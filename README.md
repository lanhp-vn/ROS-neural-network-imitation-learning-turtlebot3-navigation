# Neural Network Imitation Learning for TurtleBot3 Navigation

This project implements imitation learning for autonomous robot navigation in Gazebo using ROS. The system learns from expert trajectories generated by the `move_base` navigation stack and deploys a hybrid Neural Network + PID controller for precise navigation.

## Demo Video
Watch here: https://youtu.be/y0-IlLWVIn8

## Overview

The project applies imitation learning to robot motion control in Gazebo + ROS environment. Expert trajectories are generated using the built-in `move_base` stack, and a comprehensive dataset is automatically collected over 2.5 hours. A neural network is trained with five input features (x, y, θ, goal_x, goal_y) and deployed in a hybrid NN + sequential PID controller that runs onboard the TurtleBot3.

**Key Innovation**: The hybrid controller combines the strengths of both approaches - the NN handles long-range navigation efficiently while a classical PID controller ensures guaranteed precision for the final approach.

## Objectives

- Automatically collect diverse navigation data from TurtleBot3 in Gazebo
- Preprocess and synchronize ROS topic data for neural network training
- Train a regression neural network to imitate expert navigation behavior
- Implement a hybrid control system combining NN predictions with PID control
- Deploy the controller in ROS for autonomous navigation to user-defined setpoints

## System Architecture

```
Data Collection → Preprocessing → NN Training → Hybrid Controller → Navigation Testing
      ↓               ↓              ↓              ↓                    ↓
  goals_sender    preprocess_bag   notebook    nn_controller        motion_planner
      ↓               ↓              ↓              ↓                    ↓
  43 waypoints    training_data    model.h5    NN+PID hybrid        User input
```

## Project Structure

```
/
├── ros_workspace/
│   ├── Recorded_Robot.bag          # ROS bag with recorded data (excluded from git due to size)
│   ├── model.h5                    # Trained neural network model
│   ├── training_data.csv           # Preprocessed dataset
│   ├── scripts/
│   │   ├── preprocess_bag.py       # Extract data from ROS bag
│   │   └── read_h5py.py           # Utility to inspect HDF5 structure
│   └── src/nn_turtlebot/
│       ├── scripts/
│       │   ├── goals_sender.py      # Automated goal sender for data collection
│       │   ├── nn_controller.py     # Hybrid NN+PID controller
│       │   └── motion_planner.py    # User input for testing
│       └── package.xml
├── training_notebook.ipynb         # Neural network training (Google Colab)
└── READMORE.pdf                    # Full project documentation, methodology and results
```

## Setup Instructions

### 1. ROS Workspace Setup

1. **Create and build workspace:**
```bash
mkdir -p ~/lab8_workspace/src
cd ~/lab8_workspace
catkin_make
source devel/setup.bash
```

2. **Copy the nn_turtlebot package:**
```bash
cp -r ros_workspace/src/nn_turtlebot ~/lab8_workspace/src/
cd ~/lab8_workspace
catkin_make
```

3. **Install TurtleBot3 dependencies:**
```bash
sudo apt-get install ros-noetic-turtlebot3-*
echo "export TURTLEBOT3_MODEL=waffle_pi" >> ~/.bashrc
source ~/.bashrc
```

### 2. Model Setup

1. **Copy the trained model:**
```bash
cp ros_workspace/model.h5 ~/lab8_workspace/model.h5
```

2. **Update model path in nn_controller.py:**
```python
MODEL_FILE = "/home/ubuntu/lab8_workspace/model.h5"  # Update this path
```

## Data Collection Process

### Automated Data Collection

The data collection process uses an automated goal sender that navigates through 43 predefined waypoints:

1. **Launch Gazebo environment:**
```bash
roslaunch turtlebot3_gazebo turtlebot3_world.launch
```

2. **Launch navigation stack:**
```bash
roslaunch turtlebot3_navigation turtlebot3_navigation.launch map_file:=$HOME/map.yaml
```

3. **Start automated goal sender:**
```bash
rosrun nn_turtlebot goals_sender.py
```

4. **Record data (in separate terminal):**
```bash
rosbag record -O Recorded_Robot /gazebo/model_states /cmd_vel /move_base/goal
```

**Note**: The original `Recorded_Robot.bag` file (1.8GB) has been excluded from this repository due to GitHub's file size limits. Users can either:
- Use the preprocessed `training_data.csv` directly for training
- Record your own bag file following the above process
- Update the path in `preprocess_bag.py` if using a different bag file

### Data Collection Details

- **Duration**: 2.5 hours of continuous data collection
- **Goals**: 43 strategically placed waypoints covering the entire map
- **Data Points**: ~39,450 synchronized state-action pairs
- **Topics Recorded**:
  - `/gazebo/model_states`: Robot pose (x, y, θ)
  - `/cmd_vel`: Expert control commands (v, w)
  - `/move_base/goal`: Navigation goals (goal_x, goal_y, goal_θ)

## Data Preprocessing

Convert the recorded ROS bag to training-ready CSV format:

```bash
cd ros_workspace/scripts
python preprocess_bag.py
```

**Preprocessing Steps**:
1. **Data Extraction**: Extract robot poses, control commands, and goals
2. **Synchronization**: Align control actions with corresponding states and goals
3. **Format Conversion**: Convert quaternions to Euler angles
4. **CSV Export**: Save as `training_data.csv` with columns: [x, y, theta, goal_x, goal_y, goal_theta, v, w]

## Neural Network Training

### Training Environment: Google Colab

Upload `training_data.csv` to Google Colab and run `training_notebook.ipynb`.

### Network Architecture

```python
model = keras.Sequential([
    keras.layers.Dense(256, activation='relu', input_shape=(6,)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(2, activation='linear')  # Outputs: [v, w]
])
```

### Training Configuration

- **Input Features**: 6 (x, y, theta, goal_x, goal_y, goal_theta)
- **Output**: 2 (linear velocity v, angular velocity w)
- **Loss Function**: Mean Squared Error (MSE)
- **Optimizer**: Adam (learning_rate=0.0005)
- **Regularization**: Early stopping (patience=25)
- **Data Split**: 80% training, 20% validation
- **Total Samples**: 39,450 data points

### Training Results

- **Final Training Loss**: ~0.021
- **Final Validation Loss**: ~0.022
- **Training Time**: ~150 epochs with early stopping
- **Model Size**: 394KB (model.h5)

## Hybrid Controller Implementation

### Controller Architecture

The `nn_controller.py` implements a hybrid approach:

1. **Long-range Navigation (distance > 0.20m)**:
   - Uses trained neural network predictions
   - Input: [x, y, θ, goal_x, goal_y, 0.0]
   - Output: [v_predicted, w_predicted]

2. **Precision Docking (distance ≤ 0.20m)**:
   - Switches to sequential PID controller
   - **Phase 1**: Align to goal direction
   - **Phase 2**: Drive straight to goal
   - **Phase 3**: Final orientation alignment (θ = 0)

### PID Parameters

```python
# Linear PID gains
Kp_lin, Ki_lin, Kd_lin = 0.4, 0.0001, 0.0001

# Angular PID gains  
Kp_ang, Ki_ang, Kd_ang = 1.0, 0.0001, 0.0001
```

## Testing and Usage

### Launch the System

1. **Terminal 1 - Gazebo Environment:**
```bash
roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch
```

2. **Terminal 2 - NN Controller:**
```bash
rosrun nn_turtlebot nn_controller.py
```

3. **Terminal 3 - Motion Planner:**
```bash
rosrun nn_turtlebot motion_planner.py
```

### Usage Instructions

1. The motion planner will prompt: `Enter target X: `
2. Input desired X coordinate (e.g., `2.0`)
3. Input desired Y coordinate (e.g., `3.0`) 
4. The robot will autonomously navigate to the goal
5. Once reached, the system prompts for the next goal

### Example Session

```bash
$ rosrun nn_turtlebot motion_planner.py
motion_planner started. Enter goals (x y).
Enter target X: 2.0
Enter target Y: 3.0
Published [x=2.00, y=3.00] → waiting for /goal_reached...
Goal reached!
Enter target X: -1.5
Enter target Y: 1.0
...
```
